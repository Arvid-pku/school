{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "randitnum = 15\n",
    "def getReward(action):\n",
    "\treturn np.random.normal(action, 1)\n",
    "Q = {k:0 for k in range(1, randitnum+1)}\n",
    "aNums = {k:0 for k in range(1, randitnum+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def greedy(Q):\n",
    "\treturn np.argmax([Q[k] for k in Q]) + 1\n",
    "\n",
    "def getAction(e):\n",
    "\taction = 0\n",
    "\tif random.random() < e:\n",
    "\t\taction = random.randint(1, 15)\n",
    "\telse:\n",
    "\t\taction = greedy(Q)\n",
    "\treturn action\n",
    "def updateQ(action, reward):\n",
    "\tQ[action] = Q[action] + 1/aNums[action] * (reward - Q[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "actions = []\n",
    "def run(max_step, e):\n",
    "\tfor step in range(1, max_step+1):\n",
    "\t\taction = getAction(e)\n",
    "\t\tactions.append(action)\n",
    "\t\taNums[action] += 1\n",
    "\t\treward = getReward(action)\n",
    "\t\trewards.append(reward)\n",
    "\t\tupdateQ(action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(2000, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 53,\n",
       " 2: 68,\n",
       " 3: 66,\n",
       " 4: 64,\n",
       " 5: 50,\n",
       " 6: 41,\n",
       " 7: 58,\n",
       " 8: 63,\n",
       " 9: 54,\n",
       " 10: 45,\n",
       " 11: 41,\n",
       " 12: 47,\n",
       " 13: 44,\n",
       " 14: 56,\n",
       " 15: 1250}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.3090299899087663, 8.78932550661897, 4.427652807616279, 5.322401199441085, 1.492212354487409], [1.5216217226302908, 2.992351499157443, 2.250173575792262, 1.9076053117399359, 0.5474363022367601], [0.050856143296821144, 0.73820423214837, 0.6731468850353048, 0.35821982271759384, -0.4031075461826618], [-0.9735586507977186, -0.43546178774483035, -0.35484864168343366, -0.5855714799606567, -1.183041483445346], [-1.857666897631648, -1.3451976215169594, -1.2292336361173568, -1.4228845392602936, -1.975145450105421]]\n"
     ]
    }
   ],
   "source": [
    "values = [[0 for i in range(5)] for j in range(5)]\n",
    "def getdirect(a):\n",
    "\treturn [(-1, 0), (1, 0), (0, 1), (0, -1)][a]\n",
    "def newpos(x, y, a):\n",
    "\tdx, dy = getdirect(a)\n",
    "\tnx = x + dx\n",
    "\tny = y + dy\n",
    "\tif x == 0 and y == 1:\n",
    "\t\treturn 4, 1, 10\n",
    "\tif x == 0 and y == 3:\n",
    "\t\treturn 2, 3, 5\n",
    "\tif nx < 0 or nx >= 5 or ny < 0 or ny >= 5:\n",
    "\t\treturn x, y, -1\n",
    "\treturn nx, ny, 0\n",
    "while True:\n",
    "\tnewvalues = [[0 for i in range(5)] for j in range(5)]\n",
    "\tfor x in range(5):\n",
    "\t\tfor y in range(5):\n",
    "\t\t\tfor a in range(4):\n",
    "\t\t\t\tnx, ny, reward = newpos(x, y, a)\n",
    "\t\t\t\tnewvalues[x][y] += 0.25 * (reward + 0.9 * values[nx][ny])\n",
    "\tdelta = 0\n",
    "\tfor x in range(5):\n",
    "\t\tfor y in range(5):\n",
    "\t\t\tdelta += abs(newvalues[x][y] - values[x][y])\n",
    "\tif delta < 1e-4:\n",
    "\t\tprint(newvalues)\n",
    "\t\tbreak\n",
    "\tvalues = newvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.30902999  8.78932551  4.42765281  5.3224012   1.49221235]\n",
      " [ 1.52162172  2.9923515   2.25017358  1.90760531  0.5474363 ]\n",
      " [ 0.05085614  0.73820423  0.67314689  0.35821982 -0.40310755]\n",
      " [-0.97355865 -0.43546179 -0.35484864 -0.58557148 -1.18304148]\n",
      " [-1.8576669  -1.34519762 -1.22923364 -1.42288454 -1.97514545]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#定义grid问题中常用的变量\n",
    "# 格子尺寸\n",
    "WORLD_SIZE = 5\n",
    "# 状态A的位置(下标从0开始,下同)\n",
    "A_POS = [0, 1]\n",
    "# 状态A'的位置\n",
    "A_PRIME_POS = [4, 1]\n",
    "# 状态B的位置\n",
    "B_POS = [0, 3]\n",
    "# 状态B'的位置\n",
    "B_PRIME_POS = [2, 3]\n",
    "# 折扣因子\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "# 动作集={上,下,左,右}\n",
    "ACTIONS = [np.array([-1, 0]),\n",
    "           np.array([1, 0]),\n",
    "           np.array([0, 1]),\n",
    "           np.array([0, -1]),\n",
    "]\n",
    "# 策略,每个动作等概率\n",
    "ACTION_PROB = 0.25\n",
    "\n",
    "def step(state, action):\n",
    "  \n",
    "    # 如果当前位置为状态A,则直接跳到状态A',奖励为+10\n",
    "    if state == A_POS:\n",
    "        return A_PRIME_POS, 10\n",
    "    # 如果当前位置为状态B,则直接跳到状态B',奖励为+5\n",
    "    if state == B_POS:\n",
    "        return B_PRIME_POS, 5\n",
    "\n",
    "    state = np.array(state)\n",
    "    # 通过坐标运算得到后继状态\n",
    "    next_state = (state + action).tolist()\n",
    "    x, y = next_state\n",
    "    # 根据后继状态的坐标判断是否出界\n",
    "    if x < 0 or x >= WORLD_SIZE or y < 0 or y >= WORLD_SIZE:\n",
    "        # 出界则待在原地,奖励为-1\n",
    "        reward = -1.0\n",
    "        next_state = state\n",
    "    else:\n",
    "        # 未出界则奖励为0\n",
    "        reward = 0\n",
    "    return next_state, reward\n",
    "\n",
    "def bellman_equation():\n",
    "    ''' 求解贝尔曼(期望)方程\n",
    "    '''\n",
    "    # 初始值函数\n",
    "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    while True:\n",
    "        new_value = np.zeros(value.shape)\n",
    "        # 遍历所有状态\n",
    "        for i in range(0, WORLD_SIZE):\n",
    "            for j in range(0, WORLD_SIZE):\n",
    "                # 遍历所有动作\n",
    "                for action in ACTIONS:\n",
    "                    # 执行动作,转移到后继状态,并获得立即奖励\n",
    "                    (next_i, next_j), reward = step([i, j], action)\n",
    "                    # 贝尔曼期望方程\n",
    "                    new_value[i, j] += ACTION_PROB * \\\n",
    "                    (reward + DISCOUNT * value[next_i, next_j])\n",
    "        # 迭代终止条件: 误差小于1e-4\n",
    "        if np.sum(np.abs(value - new_value)) < 1e-4:\n",
    "            print(new_value)\n",
    "            break\n",
    "        value = new_value\n",
    "\n",
    "bellman_equation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40df05e53e1f42141473e2a45bd3b16f06596a8a1d943aa97f5eae81a0bb8714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
